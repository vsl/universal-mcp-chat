{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The imports\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from agents import Agent, Runner, trace, OpenAIChatCompletionsModel, AsyncOpenAI\n",
    "from agents.mcp import MCPServerStdio, MCPServerStreamableHttp\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mcp_params = []\n",
    "mcp_http_params = []\n",
    "mcp_servers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mcp_params.append({\"command\": \"uvx\", \"args\": [\"mcp-server-fetch\"]})\n",
    "mcp_params.append({\"command\": \"uv\", \"args\": [\"run\", \"examples/py/date_mcp_server.py\"]})\n",
    "mcp_params.append({\"command\": \"uv\", \"args\": [\"run\", \"examples/py/pvc_company.py\"]}) \n",
    "\n",
    "# mcp_http_params.append({\"url\": \"https://mcp.deepwiki.com/mcp\"})\n",
    "mcp_http_params.append({\"url\": \"http://localhost:3011/mcp\"})\n",
    "\n",
    "\n",
    "mcp_servers.extend(MCPServerStdio(params=fetch_params, client_session_timeout_seconds=120) for fetch_params in mcp_params)\n",
    "mcp_servers.extend(MCPServerStreamableHttp(params=fetch_params, client_session_timeout_seconds=120) for fetch_params in mcp_http_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for server in mcp_servers:\n",
    "        print(f\"Connecting to MCP server: {server}\")\n",
    "        await server.connect()\n",
    "        tools = await server.list_tools()\n",
    "        for tool in tools:\n",
    "            print(f\"Tool available on {server}: {tool}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modal_name = \"gemma-3-12b\"\n",
    "# modal_name = \"llama3.2\"\n",
    "# modal_name = \"gpt-4o-mini\"\n",
    "# modal_name = \"deepseek-r1-distill-qwen-7b\"\n",
    "url = \"http://localhost:1234/v1\"\n",
    "# url = \"http://localhost:11434/v1\"\n",
    "api_key = \"lm-studio\"\n",
    "# api_key = \"ollama\"\n",
    "\n",
    "load_dotenv(override=True)\n",
    "# api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Configure the model\n",
    "client = AsyncOpenAI(base_url=url, api_key=api_key)\n",
    "# client = AsyncOpenAI(api_key=api_key)\n",
    "\n",
    "model = OpenAIChatCompletionsModel(\n",
    "    model=modal_name,\n",
    "    openai_client=client\n",
    ")\n",
    "\n",
    "async def get_researcher(mcp_servers) -> Agent:\n",
    "# Create the agent\n",
    "    agent = Agent(\n",
    "        mcp_servers=mcp_servers,\n",
    "        name=\"Assistant\",\n",
    "        instructions=\"You are a helpful assistant with shortly answering questions.\",\n",
    "        model=model\n",
    "    )\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_chat(message, history):\n",
    "    print(f\"Running chat with message: {message} and history: {history}\")\n",
    "    for server in mcp_servers:\n",
    "        await server.connect()\n",
    "    print(f\"Connected to MCP servers: {mcp_servers}\")\n",
    "\n",
    "    history.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    yield history, \"\"\n",
    "\n",
    "    researcher = await get_researcher(mcp_servers)\n",
    "    \n",
    "    with trace(\"account_mcp_client\"):\n",
    "        result = await Runner.run(\n",
    "            researcher,\n",
    "            history, \n",
    "            max_turns=30,\n",
    "        )\n",
    "\n",
    "    history.append({\"role\": \"assistant\", \"content\": result.final_output})\n",
    "\n",
    "\n",
    "    yield history, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with gr.Blocks() as ui:\n",
    "    chat = gr.Chatbot(type=\"messages\", label=\"agent chat\", height=500)\n",
    "    chat_history = gr.State([])\n",
    "    \n",
    "    txt = gr.Textbox(placeholder=\"Chat with our AI Assistant:\", show_label=False)\n",
    "    btn = gr.Button(\"Run\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        clear = gr.Button(\"Clear\")\n",
    "\n",
    "    btn.click(\n",
    "        fn=run_chat,\n",
    "        inputs=[txt, chat_history],\n",
    "        outputs=[chat, txt],\n",
    "    )\n",
    "    txt.submit(\n",
    "        fn=run_chat,\n",
    "        inputs=[txt, chat_history],\n",
    "        outputs=[chat, txt],\n",
    "    )\n",
    "    clear.click(lambda: ([], []), inputs=None, outputs=[chat, chat_history], queue=False)\n",
    "\n",
    "ui.launch(inbrowser=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the trace\n",
    "\n",
    "* https://platform.openai.com/traces\n",
    "* https://www.stainless.com/mcp/how-to-test-mcp-servers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### how to test mcp \n",
    "> npx @modelcontextprotocol/inspector\n",
    "\n",
    "* Command: `uv`\n",
    "* Arguments: `run examples/py/date_mcp_server.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "1. what is the website of Vorkov PVC company? - test MCP\n",
    "2. what is my time? - test: AI must ask user name."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "universal-mcp-chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
